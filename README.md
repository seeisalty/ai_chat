# AI Chat (Django + LocalAI Mistral-7B)

A professional, ChatGPT-style Django chat frontend that streams responses from a local open-source LLM (Mistral-7B) running via LocalAI in Docker.

## Features
- Modern, ChatGPT-inspired UI (dark mode, avatars, streaming effect)
- Real-time streaming from a local Mistral-7B model (GGUF format)
- No OpenAI API required; runs fully local with Docker

---

## Quick Start

### 1. Prerequisites
- Docker
- Python 3.10+
- pip (Python package manager)

### 2. Download the Model
Place your Mistral-7B GGUF model and YAML config in the `models/` directory. Example files:
- `models/mistral-7b-v0.1.Q4_K_M.gguf`
- `models/mistral-7b-v0.1.Q4_K_M.yaml`

### 3. Build and Run LocalAI (Terminal 1)
Open a terminal and run:
```sh
# From the project root
docker build -t localai-mistral .
docker run --rm -p 5000:5000 localai-mistral
```
This will start LocalAI and serve the model on `http://localhost:5000`.

### 4. Start Django (Terminal 2)
Open a second terminal and run:
```sh
# From the project root
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt  # (create if needed: Django, requests)
python manage.py migrate
python manage.py runserver
```

### 5. Open the Chat UI
Go to [http://localhost:8000](http://localhost:8000) in your browser.

---

## Notes
- **You must keep both terminals open:**
  - One for Docker/LocalAI (model server)
  - One for Django (web frontend)
- The chat UI streams responses as they are generated by the local model.
- If you see timeouts or errors, check that LocalAI is running and the model is loaded.

---

## Troubleshooting
- If you get `[Error: ...timed out]` in the chat, LocalAI may be slow to respond or not running.
- Use a smaller or more quantized model if you have limited hardware.
- Check LocalAI logs for model loading errors.

---

## Credits
- [LocalAI](https://github.com/go-skynet/LocalAI)
- [Mistral-7B](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF)
- Django, requests

---

## License
MIT
